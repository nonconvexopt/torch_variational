{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4709797a-9db8-4650-b912-bc6201868d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from adabelief_pytorch import AdaBelief\n",
    "\n",
    "from wrapper import Variational_Flipout, Variational_LRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "5d3b7e0b-81fe-4af7-9bed-2e47644d25d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "train_set = datasets.MNIST(\n",
    "    '~/data', train=True, download=True,\n",
    "    transform=transform)\n",
    "test_set = datasets.MNIST(\n",
    "    '~/data', train=False,\n",
    "    transform=transform)\n",
    "train_loader = DataLoader(train_set, batch_size = 128, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size = 128, shuffle=False)\n",
    "\n",
    "def mul_sign(x) -> torch.Tensor:\n",
    "    #Best performance on several experiments\n",
    "    return x.mul(torch.empty(x.shape, device = x.device).uniform_(-1,1).sign())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "d6dd31cd-4952-4cd5-82cd-bfd7f34c60d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(net, self).__init__()\n",
    "        self.b1 = nn.BatchNorm2d(1)\n",
    "        self.l1 = Variational_Flipout(nn.Conv2d(1, 64, kernel_size=3))\n",
    "        self.p1 = nn.MaxPool2d((2,2))\n",
    "        self.b2 = nn.BatchNorm2d(64)\n",
    "        self.l2 = Variational_Flipout(nn.Conv2d(64, 64, kernel_size=3))\n",
    "        self.p2 = nn.MaxPool2d((2,2))\n",
    "        self.b3 = nn.BatchNorm2d(64)\n",
    "        self.l3 = Variational_Flipout(nn.Conv2d(64, 64, kernel_size=3))\n",
    "        \n",
    "        self.b4 = nn.BatchNorm1d(576)\n",
    "        self.l4 = Variational_Flipout(nn.Linear(576, 64))\n",
    "        self.b5 = nn.BatchNorm1d(64)\n",
    "        self.l5 = Variational_Flipout(nn.Linear(64, 10))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.b1(x)\n",
    "        x = self.l1(x)\n",
    "        x = F.silu(x)\n",
    "        x = self.p1(x)\n",
    "        x = self.b2(x)\n",
    "        x = self.l2(x)\n",
    "        x = F.silu(x)\n",
    "        x = self.p2(x)\n",
    "        x = self.b3(x)\n",
    "        x = self.l3(x)\n",
    "        x = F.silu(x)\n",
    "        \n",
    "        x = x.view(x.shape[0], -1)\n",
    "        \n",
    "        x = self.b4(x)\n",
    "        x = self.l4(x)\n",
    "        x = F.silu(x)\n",
    "        x = self.b5(x)\n",
    "        x = self.l5(x)\n",
    "                \n",
    "        return x\n",
    "    \n",
    "    def kld(self):\n",
    "        sum_kl = 0.0\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, Variational_Flipout):\n",
    "                sum_kl += module.kld()\n",
    "        return sum_kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "dd05bea7-5a10-46d5-a2e6-462ebf8c62c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Variational_LRT(nn.Module):\n",
    "    def __init__(self, module: nn.Module, weight_multiplcative_variance = True):\n",
    "        super(Variational_LRT, self).__init__()\n",
    "        \"\"\"\n",
    "        Wrapper class for existing torch modules.\n",
    "        Use multiplicative noise in weight space to make layer stochastic.\n",
    "        \"\"\"\n",
    "        \n",
    "        #assert True in [isinstance(module, m) for m in registered_modules]\n",
    "\n",
    "        self.weight_mean = module\n",
    "        self.weight_logvar = nn.Parameter(self.weight_mean.weight.data.clone().detach().fill_(0))\n",
    "        self.weight_multiplcative_variance = weight_multiplcative_variance\n",
    "    \n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        weight = self.weight_mean.weight    \n",
    "        bias = self.weight_mean.bias\n",
    "        self.weight_mean.bias = None\n",
    "        \n",
    "        #Assume standard normal prior of weight and calculate output variance\n",
    "        with torch.no_grad():\n",
    "            self.weight_mean.weight.data = torch.ones(\n",
    "                self.weight_mean.weight.data.shape,\n",
    "                device = self.weight_mean.weight.data.device,\n",
    "                requires_grad = False,\n",
    "            )\n",
    "            var_prior = self.weight_mean(x.pow(2))\n",
    "        \n",
    "        #Calculate LRT variance of layer output\n",
    "        if self.weight_multiplcative_variance:\n",
    "            self.weight_mean.weight.data = weight.pow(2) * self.weight_logvar.exp()\n",
    "        else:\n",
    "            self.weight_mean.weight.data = self.weight_logvar.exp()\n",
    "        var = self.weight_mean(x.pow(2))\n",
    "        \n",
    "        self.weight_mean.weight.data = weight\n",
    "        self.weight_mean.bias = bias\n",
    "        mean = self.weight_mean(x)\n",
    "        \n",
    "        self._kld = (mean.pow(2) - var + (var - var_prior).exp() - 1).mean().div(2)\n",
    "        return mean + var.sqrt() * torch.randn(var.shape, device = var.device, requires_grad = False)\n",
    "    \n",
    "    def kld(self) -> torch.Tensor:\n",
    "        return self._kld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "245b5109-6b25-4589-99bd-43eeaebbd27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(net, self).__init__()\n",
    "        self.b1 = nn.BatchNorm2d(1)\n",
    "        self.l1 = Variational_LRT(nn.Conv2d(1, 64, kernel_size=3))\n",
    "        self.p1 = nn.MaxPool2d((2,2))\n",
    "        self.b2 = nn.BatchNorm2d(64)\n",
    "        self.l2 = Variational_LRT(nn.Conv2d(64, 64, kernel_size=3))\n",
    "        self.p2 = nn.MaxPool2d((2,2))\n",
    "        self.b3 = nn.BatchNorm2d(64)\n",
    "        self.l3 = Variational_LRT(nn.Conv2d(64, 64, kernel_size=3))\n",
    "        \n",
    "        self.b4 = nn.BatchNorm1d(576)\n",
    "        self.l4 = Variational_LRT(nn.Linear(576, 64))\n",
    "        self.b5 = nn.BatchNorm1d(64)\n",
    "        self.l5 = Variational_LRT(nn.Linear(64, 10))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.b1(x)\n",
    "        x = self.l1(x)\n",
    "        x = F.silu(x)\n",
    "        x = self.p1(x)\n",
    "        x = self.b2(x)\n",
    "        x = self.l2(x)\n",
    "        x = F.silu(x)\n",
    "        x = self.p2(x)\n",
    "        x = self.b3(x)\n",
    "        x = self.l3(x)\n",
    "        x = F.silu(x)\n",
    "        \n",
    "        x = x.view(x.shape[0], -1)\n",
    "        \n",
    "        x = self.b4(x)\n",
    "        x = self.l4(x)\n",
    "        x = F.silu(x)\n",
    "        x = self.b5(x)\n",
    "        x = self.l5(x)\n",
    "                \n",
    "        return x\n",
    "    \n",
    "    def kld(self):\n",
    "        sum_kl = 0.0\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, Variational_LRT):\n",
    "                sum_kl += module.kld()\n",
    "        return sum_kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "3cbf48c4-63ad-4582-8e33-a18c0a2eb3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-pytorch from version 0.0.5.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  ---------\n",
      "adabelief-pytorch=0.0.5  1e-08  False              False\n",
      ">=0.1.0 (Current 0.2.0)  1e-16  True               True\n",
      "\u001b[34mSGD better than Adam (e.g. CNN for Image Classification)    Adam better than SGD (e.g. Transformer, GAN)\n",
      "----------------------------------------------------------  ----------------------------------------------\n",
      "Recommended eps = 1e-8                                      Recommended eps = 1e-16\n",
      "\u001b[34mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[34mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[32mYou can disable the log message by setting \"print_change_log = False\", though it is recommended to keep as a reminder.\n",
      "\u001b[0m\n",
      "Weight decoupling enabled in AdaBelief\n"
     ]
    }
   ],
   "source": [
    "model = net().cuda()\n",
    "num_epochs = 200\n",
    "lr = 3e-4\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = AdaBelief(\n",
    "    model.parameters(),\n",
    "    lr=lr, betas=(0.9, 0.999), eps=1e-8, weight_decouple = True, rectify=False, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1956c6a-e274-4cdd-a111-121b72cd9b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    50] loss: 1.876510, kld: 38.720524\n",
      "[1,   100] loss: 1.120174, kld: 32.141052\n",
      "[1,   150] loss: 0.824844, kld: 25.933521\n",
      "[1,   200] loss: 0.684259, kld: 20.661304\n",
      "[1,   250] loss: 0.537141, kld: 16.437626\n",
      "[1,   300] loss: 0.444920, kld: 13.141074\n",
      "[1,   350] loss: 0.380234, kld: 10.645747\n",
      "[1,   400] loss: 0.334852, kld: 8.800092\n",
      "[1,   450] loss: 0.288808, kld: 7.420666\n",
      "[2,    50] loss: 0.261173, kld: 6.132567\n",
      "[2,   100] loss: 0.242622, kld: 5.467493\n",
      "[2,   150] loss: 0.236099, kld: 5.007080\n",
      "[2,   200] loss: 0.227049, kld: 4.658742\n",
      "[2,   250] loss: 0.209621, kld: 4.387023\n",
      "[2,   300] loss: 0.217690, kld: 4.200820\n",
      "[2,   350] loss: 0.198898, kld: 4.047126\n",
      "[2,   400] loss: 0.198934, kld: 3.934625\n",
      "[2,   450] loss: 0.195539, kld: 3.853153\n",
      "[3,    50] loss: 0.186454, kld: 3.733509\n",
      "[3,   100] loss: 0.189648, kld: 3.676773\n",
      "[3,   150] loss: 0.182483, kld: 3.622032\n",
      "[3,   200] loss: 0.193903, kld: 3.625362\n",
      "[3,   250] loss: 0.188844, kld: 3.593967\n",
      "[3,   300] loss: 0.183582, kld: 3.559656\n",
      "[3,   350] loss: 0.172966, kld: 3.500956\n",
      "[3,   400] loss: 0.183937, kld: 3.483795\n",
      "[3,   450] loss: 0.172804, kld: 3.456946\n",
      "[4,    50] loss: 0.171835, kld: 3.426829\n",
      "[4,   100] loss: 0.179527, kld: 3.409113\n",
      "[4,   150] loss: 0.181427, kld: 3.444891\n",
      "[4,   200] loss: 0.171060, kld: 3.420128\n",
      "[4,   250] loss: 0.169299, kld: 3.389307\n",
      "[4,   300] loss: 0.173476, kld: 3.353560\n",
      "[4,   350] loss: 0.165198, kld: 3.327627\n",
      "[4,   400] loss: 0.165668, kld: 3.304596\n",
      "[4,   450] loss: 0.171796, kld: 3.278012\n",
      "[5,    50] loss: 0.169735, kld: 3.275860\n",
      "[5,   100] loss: 0.159469, kld: 3.265946\n",
      "[5,   150] loss: 0.162340, kld: 3.256758\n",
      "[5,   200] loss: 0.156986, kld: 3.227556\n",
      "[5,   250] loss: 0.166950, kld: 3.257654\n",
      "[5,   300] loss: 0.152959, kld: 3.234672\n",
      "[5,   350] loss: 0.171528, kld: 3.235041\n",
      "[5,   400] loss: 0.156639, kld: 3.208734\n",
      "[5,   450] loss: 0.158954, kld: 3.194058\n",
      "[6,    50] loss: 0.154881, kld: 3.159845\n",
      "[6,   100] loss: 0.153308, kld: 3.158219\n",
      "[6,   150] loss: 0.147789, kld: 3.157275\n",
      "[6,   200] loss: 0.141540, kld: 3.126525\n",
      "[6,   250] loss: 0.149912, kld: 3.104711\n",
      "[6,   300] loss: 0.157453, kld: 3.108356\n",
      "[6,   350] loss: 0.149576, kld: 3.117739\n",
      "[6,   400] loss: 0.147894, kld: 3.088701\n",
      "[6,   450] loss: 0.156800, kld: 3.091792\n",
      "[7,    50] loss: 0.147672, kld: 3.068813\n",
      "[7,   100] loss: 0.156316, kld: 3.085087\n",
      "[7,   150] loss: 0.144204, kld: 3.076394\n",
      "[7,   200] loss: 0.147785, kld: 3.062847\n",
      "[7,   250] loss: 0.138224, kld: 3.019647\n",
      "[7,   300] loss: 0.135790, kld: 2.981962\n",
      "[7,   350] loss: 0.155615, kld: 3.005255\n",
      "[7,   400] loss: 0.150397, kld: 2.999070\n",
      "[7,   450] loss: 0.152060, kld: 3.020743\n",
      "[8,    50] loss: 0.136174, kld: 2.973367\n",
      "[8,   100] loss: 0.142335, kld: 2.972255\n",
      "[8,   150] loss: 0.127702, kld: 2.936022\n",
      "[8,   200] loss: 0.140989, kld: 2.927460\n",
      "[8,   250] loss: 0.138896, kld: 2.949260\n",
      "[8,   300] loss: 0.142234, kld: 2.947863\n",
      "[8,   350] loss: 0.145251, kld: 2.966440\n",
      "[8,   400] loss: 0.137934, kld: 2.945909\n",
      "[8,   450] loss: 0.145533, kld: 2.930620\n",
      "[9,    50] loss: 0.139344, kld: 2.930988\n",
      "[9,   100] loss: 0.138041, kld: 2.932462\n",
      "[9,   150] loss: 0.130882, kld: 2.899168\n",
      "[9,   200] loss: 0.140496, kld: 2.903700\n",
      "[9,   250] loss: 0.133625, kld: 2.926962\n",
      "[9,   300] loss: 0.134069, kld: 2.918337\n",
      "[9,   350] loss: 0.140767, kld: 2.920915\n",
      "[9,   400] loss: 0.134203, kld: 2.921572\n",
      "[9,   450] loss: 0.136323, kld: 2.905118\n",
      "[10,    50] loss: 0.135671, kld: 2.894160\n",
      "[10,   100] loss: 0.134858, kld: 2.894270\n",
      "[10,   150] loss: 0.127056, kld: 2.877772\n",
      "[10,   200] loss: 0.130939, kld: 2.872236\n",
      "[10,   250] loss: 0.133760, kld: 2.885709\n",
      "[10,   300] loss: 0.128297, kld: 2.857301\n",
      "[10,   350] loss: 0.132151, kld: 2.870048\n",
      "[10,   400] loss: 0.129643, kld: 2.867104\n",
      "[10,   450] loss: 0.131814, kld: 2.876419\n",
      "[11,    50] loss: 0.129719, kld: 2.873504\n",
      "[11,   100] loss: 0.132931, kld: 2.861622\n",
      "[11,   150] loss: 0.134997, kld: 2.872401\n",
      "[11,   200] loss: 0.121981, kld: 2.849719\n",
      "[11,   250] loss: 0.116063, kld: 2.809975\n",
      "[11,   300] loss: 0.139452, kld: 2.839622\n",
      "[11,   350] loss: 0.127484, kld: 2.810668\n",
      "[11,   400] loss: 0.124124, kld: 2.793741\n",
      "[11,   450] loss: 0.136279, kld: 2.828715\n",
      "[12,    50] loss: 0.127236, kld: 2.805603\n",
      "[12,   100] loss: 0.120029, kld: 2.785958\n",
      "[12,   150] loss: 0.117327, kld: 2.760561\n",
      "[12,   200] loss: 0.129126, kld: 2.760745\n",
      "[12,   250] loss: 0.127496, kld: 2.766020\n",
      "[12,   300] loss: 0.134142, kld: 2.799020\n",
      "[12,   350] loss: 0.126102, kld: 2.810821\n",
      "[12,   400] loss: 0.124505, kld: 2.799117\n",
      "[12,   450] loss: 0.116457, kld: 2.754888\n",
      "[13,    50] loss: 0.119843, kld: 2.764292\n",
      "[13,   100] loss: 0.128670, kld: 2.760841\n",
      "[13,   150] loss: 0.130457, kld: 2.782880\n",
      "[13,   200] loss: 0.120769, kld: 2.763800\n",
      "[13,   250] loss: 0.123392, kld: 2.749376\n",
      "[13,   300] loss: 0.119644, kld: 2.716691\n",
      "[13,   350] loss: 0.119775, kld: 2.716227\n",
      "[13,   400] loss: 0.124971, kld: 2.736676\n",
      "[13,   450] loss: 0.131618, kld: 2.759474\n",
      "[14,    50] loss: 0.121200, kld: 2.749717\n",
      "[14,   100] loss: 0.129950, kld: 2.762387\n",
      "[14,   150] loss: 0.123998, kld: 2.772444\n",
      "[14,   200] loss: 0.122213, kld: 2.740905\n",
      "[14,   250] loss: 0.119430, kld: 2.726072\n",
      "[14,   300] loss: 0.118437, kld: 2.727586\n",
      "[14,   350] loss: 0.116142, kld: 2.721178\n",
      "[14,   400] loss: 0.110346, kld: 2.709610\n",
      "[14,   450] loss: 0.110670, kld: 2.647378\n",
      "[15,    50] loss: 0.120795, kld: 2.651233\n",
      "[15,   100] loss: 0.114267, kld: 2.662684\n",
      "[15,   150] loss: 0.119910, kld: 2.664453\n",
      "[15,   200] loss: 0.121647, kld: 2.682460\n",
      "[15,   250] loss: 0.113584, kld: 2.645548\n",
      "[15,   300] loss: 0.131767, kld: 2.672412\n",
      "[15,   350] loss: 0.120463, kld: 2.671343\n",
      "[15,   400] loss: 0.116931, kld: 2.670907\n",
      "[15,   450] loss: 0.114503, kld: 2.658186\n",
      "[16,    50] loss: 0.118653, kld: 2.675279\n",
      "[16,   100] loss: 0.117281, kld: 2.688210\n",
      "[16,   150] loss: 0.123106, kld: 2.675075\n",
      "[16,   200] loss: 0.130119, kld: 2.710743\n",
      "[16,   250] loss: 0.115940, kld: 2.692502\n",
      "[16,   300] loss: 0.122915, kld: 2.702547\n",
      "[16,   350] loss: 0.117291, kld: 2.706882\n",
      "[16,   400] loss: 0.115337, kld: 2.700660\n",
      "[16,   450] loss: 0.114149, kld: 2.682042\n",
      "[17,    50] loss: 0.112945, kld: 2.636576\n",
      "[17,   100] loss: 0.119378, kld: 2.664066\n",
      "[17,   150] loss: 0.115073, kld: 2.672408\n",
      "[17,   200] loss: 0.112342, kld: 2.663492\n",
      "[17,   250] loss: 0.113476, kld: 2.625539\n",
      "[17,   300] loss: 0.110322, kld: 2.634448\n",
      "[17,   350] loss: 0.119571, kld: 2.664330\n",
      "[17,   400] loss: 0.114551, kld: 2.652808\n",
      "[17,   450] loss: 0.120589, kld: 2.647079\n",
      "[18,    50] loss: 0.116605, kld: 2.664773\n",
      "[18,   100] loss: 0.109575, kld: 2.635952\n",
      "[18,   150] loss: 0.110638, kld: 2.603454\n",
      "[18,   200] loss: 0.114591, kld: 2.626974\n",
      "[18,   250] loss: 0.116943, kld: 2.626073\n",
      "[18,   300] loss: 0.115025, kld: 2.665132\n",
      "[18,   350] loss: 0.108700, kld: 2.631113\n",
      "[18,   400] loss: 0.115884, kld: 2.627852\n",
      "[18,   450] loss: 0.113086, kld: 2.630331\n",
      "[19,    50] loss: 0.115920, kld: 2.649256\n",
      "[19,   100] loss: 0.104941, kld: 2.622233\n",
      "[19,   150] loss: 0.108403, kld: 2.602659\n",
      "[19,   200] loss: 0.117982, kld: 2.642967\n",
      "[19,   250] loss: 0.102443, kld: 2.614201\n",
      "[19,   300] loss: 0.110068, kld: 2.605757\n",
      "[19,   350] loss: 0.111848, kld: 2.616819\n",
      "[19,   400] loss: 0.111777, kld: 2.619381\n",
      "[19,   450] loss: 0.110587, kld: 2.617553\n",
      "[20,    50] loss: 0.103643, kld: 2.605341\n",
      "[20,   100] loss: 0.110506, kld: 2.600947\n",
      "[20,   150] loss: 0.105250, kld: 2.582171\n",
      "[20,   200] loss: 0.110226, kld: 2.572269\n",
      "[20,   250] loss: 0.109077, kld: 2.589791\n",
      "[20,   300] loss: 0.118509, kld: 2.619238\n",
      "[20,   350] loss: 0.107388, kld: 2.593546\n",
      "[20,   400] loss: 0.127662, kld: 2.620569\n",
      "[20,   450] loss: 0.109776, kld: 2.601062\n",
      "[21,    50] loss: 0.113796, kld: 2.600990\n",
      "[21,   100] loss: 0.110095, kld: 2.592225\n",
      "[21,   150] loss: 0.112279, kld: 2.595400\n",
      "[21,   200] loss: 0.108716, kld: 2.579532\n",
      "[21,   250] loss: 0.113505, kld: 2.584790\n",
      "[21,   300] loss: 0.108171, kld: 2.584223\n",
      "[21,   350] loss: 0.111308, kld: 2.577334\n",
      "[21,   400] loss: 0.111191, kld: 2.598658\n",
      "[21,   450] loss: 0.116508, kld: 2.611003\n",
      "[22,    50] loss: 0.108496, kld: 2.620760\n",
      "[22,   100] loss: 0.117562, kld: 2.645273\n",
      "[22,   150] loss: 0.112937, kld: 2.640038\n",
      "[22,   200] loss: 0.111260, kld: 2.621847\n",
      "[22,   250] loss: 0.110327, kld: 2.631595\n",
      "[22,   300] loss: 0.099240, kld: 2.596336\n",
      "[22,   350] loss: 0.111439, kld: 2.573946\n",
      "[22,   400] loss: 0.104139, kld: 2.559440\n",
      "[22,   450] loss: 0.105903, kld: 2.570835\n",
      "[23,    50] loss: 0.105639, kld: 2.547353\n",
      "[23,   100] loss: 0.119754, kld: 2.589047\n",
      "[23,   150] loss: 0.112131, kld: 2.600197\n",
      "[23,   200] loss: 0.110885, kld: 2.590440\n",
      "[23,   250] loss: 0.103637, kld: 2.583785\n",
      "[23,   300] loss: 0.108286, kld: 2.580292\n",
      "[23,   350] loss: 0.101843, kld: 2.520314\n",
      "[23,   400] loss: 0.107997, kld: 2.541184\n",
      "[23,   450] loss: 0.100061, kld: 2.523592\n",
      "[24,    50] loss: 0.109344, kld: 2.543503\n",
      "[24,   100] loss: 0.117405, kld: 2.571380\n",
      "[24,   150] loss: 0.101140, kld: 2.541818\n",
      "[24,   200] loss: 0.109683, kld: 2.540198\n",
      "[24,   250] loss: 0.110844, kld: 2.562087\n",
      "[24,   300] loss: 0.107597, kld: 2.559615\n",
      "[24,   350] loss: 0.110867, kld: 2.557584\n",
      "[24,   400] loss: 0.109380, kld: 2.562457\n",
      "[24,   450] loss: 0.102344, kld: 2.569975\n",
      "[25,    50] loss: 0.106902, kld: 2.539338\n",
      "[25,   100] loss: 0.100824, kld: 2.526991\n",
      "[25,   150] loss: 0.106238, kld: 2.537023\n",
      "[25,   200] loss: 0.102299, kld: 2.518071\n",
      "[25,   250] loss: 0.107467, kld: 2.504422\n",
      "[25,   300] loss: 0.112165, kld: 2.542619\n",
      "[25,   350] loss: 0.104594, kld: 2.539594\n",
      "[25,   400] loss: 0.108901, kld: 2.564818\n",
      "[25,   450] loss: 0.102985, kld: 2.544671\n",
      "[26,    50] loss: 0.102886, kld: 2.541049\n",
      "[26,   100] loss: 0.108379, kld: 2.574639\n",
      "[26,   150] loss: 0.094984, kld: 2.537256\n",
      "[26,   200] loss: 0.111555, kld: 2.508167\n",
      "[26,   250] loss: 0.112269, kld: 2.544034\n",
      "[26,   300] loss: 0.099911, kld: 2.513494\n",
      "[26,   350] loss: 0.108683, kld: 2.494506\n",
      "[26,   400] loss: 0.099667, kld: 2.488828\n",
      "[26,   450] loss: 0.115265, kld: 2.515747\n",
      "[27,    50] loss: 0.109189, kld: 2.532858\n",
      "[27,   100] loss: 0.103568, kld: 2.538780\n",
      "[27,   150] loss: 0.107722, kld: 2.549483\n",
      "[27,   200] loss: 0.109675, kld: 2.554518\n",
      "[27,   250] loss: 0.100070, kld: 2.564776\n",
      "[27,   300] loss: 0.102530, kld: 2.541870\n",
      "[27,   350] loss: 0.107434, kld: 2.555160\n",
      "[27,   400] loss: 0.106997, kld: 2.524666\n",
      "[27,   450] loss: 0.105622, kld: 2.528767\n",
      "[28,    50] loss: 0.104418, kld: 2.527159\n",
      "[28,   100] loss: 0.101228, kld: 2.510214\n",
      "[28,   150] loss: 0.108551, kld: 2.526519\n",
      "[28,   200] loss: 0.105976, kld: 2.544740\n",
      "[28,   250] loss: 0.104235, kld: 2.518455\n",
      "[28,   300] loss: 0.112768, kld: 2.552719\n",
      "[28,   350] loss: 0.100654, kld: 2.542752\n",
      "[28,   400] loss: 0.104842, kld: 2.509927\n",
      "[28,   450] loss: 0.099662, kld: 2.500668\n",
      "[29,    50] loss: 0.098441, kld: 2.501160\n",
      "[29,   100] loss: 0.104270, kld: 2.515751\n",
      "[29,   150] loss: 0.109378, kld: 2.538517\n",
      "[29,   200] loss: 0.099784, kld: 2.527175\n",
      "[29,   250] loss: 0.103437, kld: 2.531871\n",
      "[29,   300] loss: 0.109348, kld: 2.554808\n",
      "[29,   350] loss: 0.107529, kld: 2.534577\n",
      "[29,   400] loss: 0.106346, kld: 2.536983\n",
      "[29,   450] loss: 0.097829, kld: 2.508341\n",
      "[30,    50] loss: 0.110227, kld: 2.531174\n",
      "[30,   100] loss: 0.100904, kld: 2.530428\n",
      "[30,   150] loss: 0.102530, kld: 2.507638\n",
      "[30,   200] loss: 0.099709, kld: 2.507951\n",
      "[30,   250] loss: 0.105484, kld: 2.501203\n",
      "[30,   300] loss: 0.107622, kld: 2.533647\n",
      "[30,   350] loss: 0.106675, kld: 2.556885\n",
      "[30,   400] loss: 0.105522, kld: 2.550333\n",
      "[30,   450] loss: 0.094140, kld: 2.507275\n",
      "[31,    50] loss: 0.101426, kld: 2.470364\n",
      "[31,   100] loss: 0.094736, kld: 2.464283\n",
      "[31,   150] loss: 0.102416, kld: 2.459522\n",
      "[31,   200] loss: 0.092897, kld: 2.456349\n",
      "[31,   250] loss: 0.108884, kld: 2.455009\n",
      "[31,   300] loss: 0.109118, kld: 2.486233\n",
      "[31,   350] loss: 0.099911, kld: 2.479492\n",
      "[31,   400] loss: 0.099640, kld: 2.450152\n",
      "[31,   450] loss: 0.115688, kld: 2.491574\n",
      "[32,    50] loss: 0.104402, kld: 2.533482\n",
      "[32,   100] loss: 0.097783, kld: 2.515197\n",
      "[32,   150] loss: 0.102320, kld: 2.480632\n",
      "[32,   200] loss: 0.104491, kld: 2.507118\n",
      "[32,   250] loss: 0.106067, kld: 2.509732\n",
      "[32,   300] loss: 0.102829, kld: 2.503107\n",
      "[32,   350] loss: 0.116128, kld: 2.543025\n",
      "[32,   400] loss: 0.103842, kld: 2.552801\n",
      "[32,   450] loss: 0.102714, kld: 2.565473\n",
      "[33,    50] loss: 0.103052, kld: 2.500656\n",
      "[33,   100] loss: 0.097951, kld: 2.500332\n",
      "[33,   150] loss: 0.099030, kld: 2.485123\n",
      "[33,   200] loss: 0.094702, kld: 2.475986\n",
      "[33,   250] loss: 0.104010, kld: 2.509995\n",
      "[33,   300] loss: 0.102369, kld: 2.510314\n",
      "[33,   350] loss: 0.103200, kld: 2.504464\n",
      "[33,   400] loss: 0.103625, kld: 2.497868\n",
      "[33,   450] loss: 0.095797, kld: 2.478412\n",
      "[34,    50] loss: 0.100418, kld: 2.465469\n",
      "[34,   100] loss: 0.101164, kld: 2.496525\n",
      "[34,   150] loss: 0.099985, kld: 2.498884\n",
      "[34,   200] loss: 0.109898, kld: 2.517422\n",
      "[34,   250] loss: 0.101912, kld: 2.509077\n",
      "[34,   300] loss: 0.097808, kld: 2.494570\n",
      "[34,   350] loss: 0.107960, kld: 2.541117\n",
      "[34,   400] loss: 0.111700, kld: 2.553042\n",
      "[34,   450] loss: 0.103965, kld: 2.541918\n",
      "[35,    50] loss: 0.095223, kld: 2.516875\n",
      "[35,   100] loss: 0.095046, kld: 2.484248\n",
      "[35,   150] loss: 0.104072, kld: 2.495739\n",
      "[35,   200] loss: 0.100006, kld: 2.482390\n",
      "[35,   250] loss: 0.095227, kld: 2.452412\n",
      "[35,   300] loss: 0.095690, kld: 2.464697\n",
      "[35,   350] loss: 0.099961, kld: 2.471876\n",
      "[35,   400] loss: 0.098954, kld: 2.458093\n",
      "[35,   450] loss: 0.106980, kld: 2.463263\n",
      "[36,    50] loss: 0.095713, kld: 2.485242\n",
      "[36,   100] loss: 0.099781, kld: 2.485127\n",
      "[36,   150] loss: 0.105655, kld: 2.485549\n",
      "[36,   200] loss: 0.100982, kld: 2.464402\n",
      "[36,   250] loss: 0.096990, kld: 2.454058\n",
      "[36,   300] loss: 0.094818, kld: 2.454217\n",
      "[36,   350] loss: 0.102570, kld: 2.464884\n",
      "[36,   400] loss: 0.102720, kld: 2.472961\n",
      "[36,   450] loss: 0.105235, kld: 2.488228\n",
      "[37,    50] loss: 0.095816, kld: 2.464026\n",
      "[37,   100] loss: 0.095125, kld: 2.446704\n",
      "[37,   150] loss: 0.095459, kld: 2.438684\n",
      "[37,   200] loss: 0.100332, kld: 2.442719\n",
      "[37,   250] loss: 0.098549, kld: 2.450606\n",
      "[37,   300] loss: 0.092635, kld: 2.413671\n",
      "[37,   350] loss: 0.105900, kld: 2.441828\n",
      "[37,   400] loss: 0.106270, kld: 2.464161\n",
      "[37,   450] loss: 0.095399, kld: 2.473995\n",
      "[38,    50] loss: 0.094565, kld: 2.475391\n",
      "[38,   100] loss: 0.100141, kld: 2.463714\n",
      "[38,   150] loss: 0.093624, kld: 2.455507\n",
      "[38,   200] loss: 0.104168, kld: 2.466845\n",
      "[38,   250] loss: 0.097665, kld: 2.470807\n",
      "[38,   300] loss: 0.095015, kld: 2.455686\n",
      "[38,   350] loss: 0.096984, kld: 2.434033\n",
      "[38,   400] loss: 0.095395, kld: 2.441399\n",
      "[38,   450] loss: 0.101946, kld: 2.453061\n",
      "[39,    50] loss: 0.093498, kld: 2.451381\n",
      "[39,   100] loss: 0.091588, kld: 2.441526\n",
      "[39,   150] loss: 0.090748, kld: 2.440760\n",
      "[39,   200] loss: 0.102007, kld: 2.458030\n",
      "[39,   250] loss: 0.106842, kld: 2.489502\n",
      "[39,   300] loss: 0.103159, kld: 2.497174\n",
      "[39,   350] loss: 0.097580, kld: 2.487244\n",
      "[39,   400] loss: 0.101813, kld: 2.503587\n",
      "[39,   450] loss: 0.101069, kld: 2.482290\n",
      "[40,    50] loss: 0.098834, kld: 2.489675\n",
      "[40,   100] loss: 0.096379, kld: 2.485412\n",
      "[40,   150] loss: 0.097086, kld: 2.452308\n",
      "[40,   200] loss: 0.101570, kld: 2.470473\n",
      "[40,   250] loss: 0.088530, kld: 2.462242\n",
      "[40,   300] loss: 0.091605, kld: 2.441897\n",
      "[40,   350] loss: 0.099743, kld: 2.452568\n",
      "[40,   400] loss: 0.096160, kld: 2.457472\n",
      "[40,   450] loss: 0.100138, kld: 2.449223\n",
      "[41,    50] loss: 0.110403, kld: 2.487610\n",
      "[41,   100] loss: 0.093734, kld: 2.499884\n",
      "[41,   150] loss: 0.091815, kld: 2.491453\n",
      "[41,   200] loss: 0.111184, kld: 2.515182\n",
      "[41,   250] loss: 0.099748, kld: 2.507619\n",
      "[41,   300] loss: 0.097367, kld: 2.489150\n",
      "[41,   350] loss: 0.090950, kld: 2.470600\n",
      "[41,   400] loss: 0.099023, kld: 2.490113\n",
      "[41,   450] loss: 0.104146, kld: 2.505648\n",
      "[42,    50] loss: 0.102947, kld: 2.510398\n",
      "[42,   100] loss: 0.101258, kld: 2.529078\n",
      "[42,   150] loss: 0.091109, kld: 2.497033\n",
      "[42,   200] loss: 0.105574, kld: 2.506843\n",
      "[42,   250] loss: 0.093704, kld: 2.485816\n",
      "[42,   300] loss: 0.100575, kld: 2.467520\n",
      "[42,   350] loss: 0.101133, kld: 2.477444\n",
      "[42,   400] loss: 0.100195, kld: 2.493601\n",
      "[42,   450] loss: 0.089257, kld: 2.471498\n",
      "[43,    50] loss: 0.097296, kld: 2.456499\n",
      "[43,   100] loss: 0.098506, kld: 2.433331\n",
      "[43,   150] loss: 0.097657, kld: 2.450043\n",
      "[43,   200] loss: 0.101942, kld: 2.465311\n",
      "[43,   250] loss: 0.094370, kld: 2.443894\n",
      "[43,   300] loss: 0.102717, kld: 2.446894\n",
      "[43,   350] loss: 0.096665, kld: 2.453732\n",
      "[43,   400] loss: 0.089568, kld: 2.432798\n",
      "[43,   450] loss: 0.098226, kld: 2.454725\n",
      "[44,    50] loss: 0.100099, kld: 2.458018\n",
      "[44,   100] loss: 0.093278, kld: 2.447083\n",
      "[44,   150] loss: 0.088869, kld: 2.421867\n",
      "[44,   200] loss: 0.093696, kld: 2.409284\n",
      "[44,   250] loss: 0.095855, kld: 2.406087\n",
      "[44,   300] loss: 0.091482, kld: 2.409427\n",
      "[44,   350] loss: 0.101466, kld: 2.431048\n",
      "[44,   400] loss: 0.092774, kld: 2.424087\n",
      "[44,   450] loss: 0.105940, kld: 2.463504\n",
      "[45,    50] loss: 0.092486, kld: 2.431840\n",
      "[45,   100] loss: 0.097825, kld: 2.435762\n",
      "[45,   150] loss: 0.095918, kld: 2.448127\n",
      "[45,   200] loss: 0.104065, kld: 2.448670\n",
      "[45,   250] loss: 0.104037, kld: 2.490322\n",
      "[45,   300] loss: 0.100795, kld: 2.512328\n",
      "[45,   350] loss: 0.101966, kld: 2.515497\n",
      "[45,   400] loss: 0.094882, kld: 2.476522\n",
      "[45,   450] loss: 0.095636, kld: 2.443826\n",
      "[46,    50] loss: 0.090322, kld: 2.409479\n",
      "[46,   100] loss: 0.095391, kld: 2.418472\n",
      "[46,   150] loss: 0.100728, kld: 2.422061\n",
      "[46,   200] loss: 0.097674, kld: 2.429241\n",
      "[46,   250] loss: 0.098417, kld: 2.453854\n",
      "[46,   300] loss: 0.096391, kld: 2.429921\n",
      "[46,   350] loss: 0.098280, kld: 2.444869\n",
      "[46,   400] loss: 0.097827, kld: 2.437801\n",
      "[46,   450] loss: 0.094042, kld: 2.399639\n",
      "[47,    50] loss: 0.100258, kld: 2.422563\n",
      "[47,   100] loss: 0.100141, kld: 2.445691\n",
      "[47,   150] loss: 0.085733, kld: 2.413123\n",
      "[47,   200] loss: 0.091043, kld: 2.390690\n",
      "[47,   250] loss: 0.094558, kld: 2.412169\n",
      "[47,   300] loss: 0.097181, kld: 2.414098\n",
      "[47,   350] loss: 0.103526, kld: 2.438230\n",
      "[47,   400] loss: 0.089177, kld: 2.420887\n",
      "[47,   450] loss: 0.091807, kld: 2.401486\n",
      "[48,    50] loss: 0.094746, kld: 2.411163\n",
      "[48,   100] loss: 0.103888, kld: 2.464288\n",
      "[48,   150] loss: 0.104241, kld: 2.473320\n",
      "[48,   200] loss: 0.094107, kld: 2.490541\n",
      "[48,   250] loss: 0.090824, kld: 2.457000\n",
      "[48,   300] loss: 0.087423, kld: 2.415880\n",
      "[48,   350] loss: 0.087731, kld: 2.393436\n",
      "[48,   400] loss: 0.101184, kld: 2.404197\n",
      "[48,   450] loss: 0.089323, kld: 2.412939\n",
      "[49,    50] loss: 0.087791, kld: 2.413900\n",
      "[49,   100] loss: 0.099561, kld: 2.424783\n",
      "[49,   150] loss: 0.091796, kld: 2.425378\n",
      "[49,   200] loss: 0.105483, kld: 2.428669\n",
      "[49,   250] loss: 0.092064, kld: 2.408159\n",
      "[49,   300] loss: 0.091905, kld: 2.385752\n",
      "[49,   350] loss: 0.098333, kld: 2.422651\n",
      "[49,   400] loss: 0.092092, kld: 2.409030\n",
      "[49,   450] loss: 0.096912, kld: 2.396897\n",
      "[50,    50] loss: 0.108435, kld: 2.450493\n",
      "[50,   100] loss: 0.097984, kld: 2.458833\n",
      "[50,   150] loss: 0.091173, kld: 2.442589\n",
      "[50,   200] loss: 0.091133, kld: 2.441822\n",
      "[50,   250] loss: 0.095247, kld: 2.433593\n",
      "[50,   300] loss: 0.092944, kld: 2.422135\n",
      "[50,   350] loss: 0.091813, kld: 2.404709\n",
      "[50,   400] loss: 0.095390, kld: 2.390357\n",
      "[50,   450] loss: 0.101710, kld: 2.412104\n",
      "[51,    50] loss: 0.089188, kld: 2.410820\n",
      "[51,   100] loss: 0.090035, kld: 2.387157\n",
      "[51,   150] loss: 0.094027, kld: 2.418016\n",
      "[51,   200] loss: 0.107457, kld: 2.457025\n",
      "[51,   250] loss: 0.091416, kld: 2.435171\n",
      "[51,   300] loss: 0.087412, kld: 2.409600\n",
      "[51,   350] loss: 0.090678, kld: 2.385284\n",
      "[51,   400] loss: 0.108417, kld: 2.437801\n",
      "[51,   450] loss: 0.097524, kld: 2.436586\n",
      "[52,    50] loss: 0.084101, kld: 2.376802\n",
      "[52,   100] loss: 0.092182, kld: 2.393982\n",
      "[52,   150] loss: 0.092412, kld: 2.380680\n",
      "[52,   200] loss: 0.100142, kld: 2.447680\n",
      "[52,   250] loss: 0.099897, kld: 2.458359\n",
      "[52,   300] loss: 0.090523, kld: 2.448197\n",
      "[52,   350] loss: 0.099679, kld: 2.443326\n",
      "[52,   400] loss: 0.084179, kld: 2.408621\n",
      "[52,   450] loss: 0.096942, kld: 2.404367\n",
      "[53,    50] loss: 0.099462, kld: 2.428283\n",
      "[53,   100] loss: 0.093611, kld: 2.441013\n",
      "[53,   150] loss: 0.091345, kld: 2.411021\n",
      "[53,   200] loss: 0.095488, kld: 2.432665\n",
      "[53,   250] loss: 0.081833, kld: 2.404521\n",
      "[53,   300] loss: 0.094658, kld: 2.415298\n",
      "[53,   350] loss: 0.094722, kld: 2.402034\n",
      "[53,   400] loss: 0.097337, kld: 2.404651\n",
      "[53,   450] loss: 0.088442, kld: 2.405989\n",
      "[54,    50] loss: 0.092196, kld: 2.411702\n",
      "[54,   100] loss: 0.089994, kld: 2.390688\n",
      "[54,   150] loss: 0.093898, kld: 2.365362\n",
      "[54,   200] loss: 0.099605, kld: 2.413910\n",
      "[54,   250] loss: 0.090811, kld: 2.403856\n",
      "[54,   300] loss: 0.104463, kld: 2.447468\n",
      "[54,   350] loss: 0.096414, kld: 2.448238\n",
      "[54,   400] loss: 0.086823, kld: 2.427330\n",
      "[54,   450] loss: 0.096443, kld: 2.418576\n",
      "[55,    50] loss: 0.085582, kld: 2.403462\n",
      "[55,   100] loss: 0.099145, kld: 2.409736\n",
      "[55,   150] loss: 0.089863, kld: 2.397239\n",
      "[55,   200] loss: 0.092690, kld: 2.386215\n",
      "[55,   250] loss: 0.103943, kld: 2.442709\n",
      "[55,   300] loss: 0.099831, kld: 2.485283\n",
      "[55,   350] loss: 0.094433, kld: 2.460881\n",
      "[55,   400] loss: 0.087665, kld: 2.423380\n",
      "[55,   450] loss: 0.092680, kld: 2.386932\n",
      "[56,    50] loss: 0.088561, kld: 2.411284\n",
      "[56,   100] loss: 0.093637, kld: 2.403950\n",
      "[56,   150] loss: 0.086489, kld: 2.402540\n",
      "[56,   200] loss: 0.092609, kld: 2.422051\n",
      "[56,   250] loss: 0.091666, kld: 2.413417\n",
      "[56,   300] loss: 0.105872, kld: 2.440630\n",
      "[56,   350] loss: 0.085194, kld: 2.417407\n",
      "[56,   400] loss: 0.097936, kld: 2.439020\n",
      "[56,   450] loss: 0.101602, kld: 2.470377\n",
      "[57,    50] loss: 0.085901, kld: 2.443968\n",
      "[57,   100] loss: 0.096964, kld: 2.456340\n",
      "[57,   150] loss: 0.097252, kld: 2.462082\n",
      "[57,   200] loss: 0.095568, kld: 2.451998\n",
      "[57,   250] loss: 0.099605, kld: 2.442075\n",
      "[57,   300] loss: 0.105016, kld: 2.490479\n",
      "[57,   350] loss: 0.087109, kld: 2.455369\n",
      "[57,   400] loss: 0.095376, kld: 2.422102\n",
      "[57,   450] loss: 0.101412, kld: 2.447444\n",
      "[58,    50] loss: 0.079080, kld: 2.380888\n",
      "[58,   100] loss: 0.090187, kld: 2.368145\n",
      "[58,   150] loss: 0.091138, kld: 2.389565\n",
      "[58,   200] loss: 0.087201, kld: 2.390533\n",
      "[58,   250] loss: 0.086154, kld: 2.372576\n",
      "[58,   300] loss: 0.086101, kld: 2.362397\n",
      "[58,   350] loss: 0.098065, kld: 2.382282\n",
      "[58,   400] loss: 0.095734, kld: 2.413031\n",
      "[58,   450] loss: 0.097390, kld: 2.431710\n",
      "[59,    50] loss: 0.097046, kld: 2.459436\n",
      "[59,   100] loss: 0.086252, kld: 2.435009\n",
      "[59,   150] loss: 0.087172, kld: 2.397118\n",
      "[59,   200] loss: 0.090937, kld: 2.406841\n",
      "[59,   250] loss: 0.101411, kld: 2.404792\n",
      "[59,   300] loss: 0.096089, kld: 2.413647\n",
      "[59,   350] loss: 0.088431, kld: 2.420631\n",
      "[59,   400] loss: 0.092235, kld: 2.386916\n",
      "[59,   450] loss: 0.091553, kld: 2.394891\n",
      "[60,    50] loss: 0.102561, kld: 2.456855\n",
      "[60,   100] loss: 0.092223, kld: 2.455727\n",
      "[60,   150] loss: 0.096583, kld: 2.454136\n",
      "[60,   200] loss: 0.094922, kld: 2.441617\n",
      "[60,   250] loss: 0.086197, kld: 2.420629\n",
      "[60,   300] loss: 0.087226, kld: 2.408352\n",
      "[60,   350] loss: 0.094934, kld: 2.385887\n",
      "[60,   400] loss: 0.087290, kld: 2.378738\n",
      "[60,   450] loss: 0.095984, kld: 2.405984\n",
      "[61,    50] loss: 0.093971, kld: 2.420447\n",
      "[61,   100] loss: 0.106827, kld: 2.480316\n",
      "[61,   150] loss: 0.087987, kld: 2.439687\n",
      "[61,   200] loss: 0.092979, kld: 2.409795\n",
      "[61,   250] loss: 0.084670, kld: 2.405242\n",
      "[61,   300] loss: 0.088037, kld: 2.392299\n",
      "[61,   350] loss: 0.096403, kld: 2.421498\n",
      "[61,   400] loss: 0.089361, kld: 2.424201\n",
      "[61,   450] loss: 0.093490, kld: 2.397218\n",
      "[62,    50] loss: 0.092588, kld: 2.429647\n",
      "[62,   100] loss: 0.079625, kld: 2.377595\n",
      "[62,   150] loss: 0.089500, kld: 2.370551\n",
      "[62,   200] loss: 0.085053, kld: 2.367114\n",
      "[62,   250] loss: 0.093109, kld: 2.375692\n",
      "[62,   300] loss: 0.096463, kld: 2.395712\n",
      "[62,   350] loss: 0.099569, kld: 2.422848\n",
      "[62,   400] loss: 0.105724, kld: 2.483038\n",
      "[62,   450] loss: 0.088275, kld: 2.476808\n",
      "[63,    50] loss: 0.084910, kld: 2.412056\n",
      "[63,   100] loss: 0.100645, kld: 2.428672\n",
      "[63,   150] loss: 0.094114, kld: 2.435186\n",
      "[63,   200] loss: 0.084772, kld: 2.414825\n",
      "[63,   250] loss: 0.101783, kld: 2.418356\n",
      "[63,   300] loss: 0.093897, kld: 2.415196\n",
      "[63,   350] loss: 0.089755, kld: 2.410095\n",
      "[63,   400] loss: 0.092864, kld: 2.421034\n",
      "[63,   450] loss: 0.090051, kld: 2.398970\n",
      "[64,    50] loss: 0.087147, kld: 2.401259\n",
      "[64,   100] loss: 0.094829, kld: 2.397575\n",
      "[64,   150] loss: 0.092445, kld: 2.414509\n",
      "[64,   200] loss: 0.089449, kld: 2.406553\n",
      "[64,   250] loss: 0.102988, kld: 2.440477\n",
      "[64,   300] loss: 0.095733, kld: 2.459114\n",
      "[64,   350] loss: 0.096215, kld: 2.441576\n",
      "[64,   400] loss: 0.101326, kld: 2.420992\n",
      "[64,   450] loss: 0.086156, kld: 2.395716\n",
      "[65,    50] loss: 0.092060, kld: 2.416831\n",
      "[65,   100] loss: 0.093592, kld: 2.430187\n",
      "[65,   150] loss: 0.088694, kld: 2.427668\n",
      "[65,   200] loss: 0.096250, kld: 2.405910\n",
      "[65,   250] loss: 0.086869, kld: 2.382695\n",
      "[65,   300] loss: 0.092493, kld: 2.366457\n",
      "[65,   350] loss: 0.095174, kld: 2.376028\n",
      "[65,   400] loss: 0.092935, kld: 2.377195\n",
      "[65,   450] loss: 0.099963, kld: 2.419186\n",
      "[66,    50] loss: 0.091462, kld: 2.441862\n",
      "[66,   100] loss: 0.089598, kld: 2.410060\n",
      "[66,   150] loss: 0.087580, kld: 2.404776\n",
      "[66,   200] loss: 0.080591, kld: 2.362420\n",
      "[66,   250] loss: 0.089853, kld: 2.354164\n",
      "[66,   300] loss: 0.089370, kld: 2.367411\n",
      "[66,   350] loss: 0.094586, kld: 2.391564\n",
      "[66,   400] loss: 0.099468, kld: 2.420700\n",
      "[66,   450] loss: 0.087045, kld: 2.389815\n",
      "[67,    50] loss: 0.091221, kld: 2.369967\n",
      "[67,   100] loss: 0.087898, kld: 2.355319\n",
      "[67,   150] loss: 0.092871, kld: 2.353543\n",
      "[67,   200] loss: 0.101725, kld: 2.410529\n",
      "[67,   250] loss: 0.085406, kld: 2.415288\n",
      "[67,   300] loss: 0.093789, kld: 2.408939\n",
      "[67,   350] loss: 0.097223, kld: 2.414418\n",
      "[67,   400] loss: 0.093880, kld: 2.408230\n",
      "[67,   450] loss: 0.091221, kld: 2.431282\n",
      "[68,    50] loss: 0.099484, kld: 2.472170\n",
      "[68,   100] loss: 0.094200, kld: 2.461972\n",
      "[68,   150] loss: 0.089154, kld: 2.441815\n",
      "[68,   200] loss: 0.096421, kld: 2.446897\n",
      "[68,   250] loss: 0.088836, kld: 2.422817\n",
      "[68,   300] loss: 0.088422, kld: 2.409148\n",
      "[68,   350] loss: 0.084267, kld: 2.411209\n",
      "[68,   400] loss: 0.094255, kld: 2.412959\n",
      "[68,   450] loss: 0.095498, kld: 2.427701\n",
      "[69,    50] loss: 0.089190, kld: 2.405051\n",
      "[69,   100] loss: 0.088762, kld: 2.383870\n",
      "[69,   150] loss: 0.092169, kld: 2.384765\n",
      "[69,   200] loss: 0.097922, kld: 2.394880\n",
      "[69,   250] loss: 0.091445, kld: 2.417644\n",
      "[69,   300] loss: 0.086117, kld: 2.384409\n",
      "[69,   350] loss: 0.091901, kld: 2.373536\n",
      "[69,   400] loss: 0.083768, kld: 2.378488\n",
      "[69,   450] loss: 0.093850, kld: 2.388383\n",
      "[70,    50] loss: 0.101637, kld: 2.436620\n",
      "[70,   100] loss: 0.095008, kld: 2.428560\n",
      "[70,   150] loss: 0.093726, kld: 2.420283\n",
      "[70,   200] loss: 0.098920, kld: 2.434457\n",
      "[70,   250] loss: 0.087562, kld: 2.400697\n",
      "[70,   300] loss: 0.093564, kld: 2.376340\n",
      "[70,   350] loss: 0.081210, kld: 2.342284\n",
      "[70,   400] loss: 0.094855, kld: 2.368561\n",
      "[70,   450] loss: 0.095007, kld: 2.386851\n",
      "[71,    50] loss: 0.090706, kld: 2.394111\n",
      "[71,   100] loss: 0.082768, kld: 2.370374\n",
      "[71,   150] loss: 0.083416, kld: 2.359469\n",
      "[71,   200] loss: 0.101361, kld: 2.400549\n",
      "[71,   250] loss: 0.087391, kld: 2.394056\n",
      "[71,   300] loss: 0.091255, kld: 2.378395\n",
      "[71,   350] loss: 0.094407, kld: 2.394597\n",
      "[71,   400] loss: 0.096342, kld: 2.411637\n",
      "[71,   450] loss: 0.081324, kld: 2.385652\n",
      "[72,    50] loss: 0.087745, kld: 2.378226\n",
      "[72,   100] loss: 0.089692, kld: 2.364460\n",
      "[72,   150] loss: 0.090803, kld: 2.386448\n",
      "[72,   200] loss: 0.090685, kld: 2.368550\n",
      "[72,   250] loss: 0.097807, kld: 2.368550\n",
      "[72,   300] loss: 0.085145, kld: 2.346706\n",
      "[72,   350] loss: 0.094036, kld: 2.374861\n",
      "[72,   400] loss: 0.091047, kld: 2.394886\n",
      "[72,   450] loss: 0.097828, kld: 2.403878\n",
      "[73,    50] loss: 0.092126, kld: 2.472426\n",
      "[73,   100] loss: 0.094820, kld: 2.462175\n",
      "[73,   150] loss: 0.089325, kld: 2.462096\n",
      "[73,   200] loss: 0.080063, kld: 2.394505\n",
      "[73,   250] loss: 0.089240, kld: 2.402121\n",
      "[73,   300] loss: 0.094979, kld: 2.421834\n",
      "[73,   350] loss: 0.084311, kld: 2.383429\n",
      "[73,   400] loss: 0.093301, kld: 2.395820\n",
      "[73,   450] loss: 0.095228, kld: 2.424706\n",
      "[74,    50] loss: 0.084987, kld: 2.451692\n",
      "[74,   100] loss: 0.093488, kld: 2.411568\n",
      "[74,   150] loss: 0.092686, kld: 2.436641\n",
      "[74,   200] loss: 0.088384, kld: 2.431734\n",
      "[74,   250] loss: 0.089990, kld: 2.409677\n",
      "[74,   300] loss: 0.093068, kld: 2.398923\n",
      "[74,   350] loss: 0.104674, kld: 2.429119\n",
      "[74,   400] loss: 0.090013, kld: 2.424059\n",
      "[74,   450] loss: 0.092180, kld: 2.402132\n",
      "[75,    50] loss: 0.093970, kld: 2.415348\n",
      "[75,   100] loss: 0.082428, kld: 2.410532\n",
      "[75,   150] loss: 0.088297, kld: 2.380027\n",
      "[75,   200] loss: 0.091168, kld: 2.392883\n",
      "[75,   250] loss: 0.090283, kld: 2.396470\n",
      "[75,   300] loss: 0.100278, kld: 2.434786\n",
      "[75,   350] loss: 0.093529, kld: 2.446028\n",
      "[75,   400] loss: 0.090825, kld: 2.413449\n"
     ]
    }
   ],
   "source": [
    "print_interval = 50\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    total_loss = 0.0\n",
    "    for ind, (x, y) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model(x.cuda()), y.cuda())\n",
    "        kld = model.kld()\n",
    "        #loss.backward()\n",
    "        (loss + 0.1*kld).backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        total_loss += loss.item()\n",
    "        if ind % print_interval == print_interval - 1:\n",
    "            print('[%d, %5d] loss: %.6f, kld: %.6f, acc: %.6f' %\n",
    "                  (epoch + 1, ind + 1, running_loss / print_interval, kld))\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0963d658-9ab4-4fda-a0b4-737c07663bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.l1.weight_logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1dcf8c-c24e-4e5e-a478-964effbceaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    total_loss = 0.0\n",
    "    for i, data in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        model_state_dict = copy.deepcopy(net_fn.state_dict())\n",
    "        #loss = - log_posterior_fn(model_state_dict, data)\n",
    "        loss = - log_posterior_fn(net_fn, model_state_dict, data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler1.step()\n",
    "        running_loss += loss.item()\n",
    "        total_loss += loss.item()\n",
    "        if i % 100 == 99:    # print every 100 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "    model_state_dict = copy.deepcopy(net_fn.state_dict())\n",
    "    test_acc, all_test_probs = evaluate_fn(test_loader, model_state_dict)\n",
    "    scheduler2.step(test_acc)\n",
    "    \n",
    "    print(\"Epoch {}\".format(epoch))\n",
    "    print(\"\\tAverage loss: {}\".format(total_loss / epoch_steps))\n",
    "    print(\"\\tTest accuracy: {}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d920f8c2-3cc7-42c8-a7b2-3d7e0bdc1aa7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
