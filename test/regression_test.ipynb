{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4709797a-9db8-4650-b912-bc6201868d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from adabelief_pytorch import AdaBelief\n",
    "\n",
    "from wrapper import Variational_Flipout, Variational_LRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "810a9b38-c669-4375-8a9f-d7ce2c01ecd8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nonconvexopt/.local/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this case special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows:\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and:\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "    \n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "data = load_boston()\n",
    "\n",
    "def standarize(x):\n",
    "    return (x - x.mean(0, keepdims = True)) / x.std(0, keepdims = True)\n",
    "\n",
    "X = data['data']\n",
    "Y = data['target']\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state=5)\n",
    "X_train = standarize(torch.from_numpy(X_train)).to(torch.float32)\n",
    "Y_train = standarize(torch.from_numpy(Y_train)).to(torch.float32)\n",
    "X_test = standarize(torch.from_numpy(X_test)).to(torch.float32)\n",
    "Y_test = standarize(torch.from_numpy(Y_test)).to(torch.float32)\n",
    "\n",
    "train_set = TensorDataset(X_train, Y_train)\n",
    "test_set = TensorDataset(X_test, Y_test)\n",
    "train_loader = DataLoader(train_set, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6dd31cd-4952-4cd5-82cd-bfd7f34c60d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class net_Flipout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(net_Flipout, self).__init__()\n",
    "        self.l1 = Variational_Flipout(nn.Linear(13, 50))\n",
    "        self.l2 = Variational_Flipout(nn.Linear(50, 1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.l1(x)\n",
    "        x = F.silu(x)\n",
    "        x = self.l2(x)\n",
    "        return x\n",
    "    \n",
    "    def kld(self):\n",
    "        sum_kl = 0.0\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, Variational_Flipout):\n",
    "                sum_kl += module.kld()\n",
    "        return sum_kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "3cbf48c4-63ad-4582-8e33-a18c0a2eb3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-pytorch from version 0.0.5.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  ---------\n",
      "adabelief-pytorch=0.0.5  1e-08  False              False\n",
      ">=0.1.0 (Current 0.2.0)  1e-16  True               True\n",
      "\u001b[34mSGD better than Adam (e.g. CNN for Image Classification)    Adam better than SGD (e.g. Transformer, GAN)\n",
      "----------------------------------------------------------  ----------------------------------------------\n",
      "Recommended eps = 1e-8                                      Recommended eps = 1e-16\n",
      "\u001b[34mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[34mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[32mYou can disable the log message by setting \"print_change_log = False\", though it is recommended to keep as a reminder.\n",
      "\u001b[0m\n",
      "Weight decoupling enabled in AdaBelief\n"
     ]
    }
   ],
   "source": [
    "model = net_Flipout().cuda()\n",
    "num_epochs = 200\n",
    "lr = 1e-2\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = AdaBelief(\n",
    "    model.parameters(),\n",
    "    lr=lr, betas=(0.9, 0.999), eps=1e-8, weight_decouple = True, rectify=False, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "245b5109-6b25-4589-99bd-43eeaebbd27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class net_LRT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(net_LRT, self).__init__()\n",
    "        self.l1 = Variational_LRT(nn.Linear(13, 50))\n",
    "        self.l2 = Variational_LRT(nn.Linear(50, 1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.l1(x)\n",
    "        x = F.silu(x)\n",
    "        x = self.l2(x)\n",
    "        return x\n",
    "    \n",
    "    def kld(self):\n",
    "        sum_kl = 0.0\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, Variational_LRT):\n",
    "                sum_kl += module.kld()\n",
    "        return sum_kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "e1956c6a-e274-4cdd-a111-121b72cd9b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     2] loss: 0.044341, kld: 7.966604\n",
      "[2,     2] loss: 0.026021, kld: 0.456928\n",
      "[3,     2] loss: 0.024317, kld: 0.283378\n",
      "[4,     2] loss: 0.018345, kld: 0.317281\n",
      "[5,     2] loss: 0.033388, kld: 0.341864\n",
      "[6,     2] loss: 0.012024, kld: 0.341758\n",
      "[7,     2] loss: 0.012887, kld: 0.303652\n",
      "[8,     2] loss: 0.030146, kld: 0.311387\n",
      "[9,     2] loss: 0.023699, kld: 0.279556\n",
      "[10,     2] loss: 0.031999, kld: 0.298626\n",
      "[11,     2] loss: 0.018676, kld: 0.323895\n",
      "[12,     2] loss: 0.024932, kld: 0.345561\n",
      "[13,     2] loss: 0.042010, kld: 0.308149\n",
      "[14,     2] loss: 0.031004, kld: 0.339616\n",
      "[15,     2] loss: 0.023130, kld: 0.219635\n",
      "[16,     2] loss: 0.023056, kld: 0.330282\n",
      "[17,     2] loss: 0.009161, kld: 0.330651\n",
      "[18,     2] loss: 0.013812, kld: 0.270136\n",
      "[19,     2] loss: 0.025652, kld: 0.334141\n",
      "[20,     2] loss: 0.029415, kld: 0.347452\n",
      "[21,     2] loss: 0.028381, kld: 0.296811\n",
      "[22,     2] loss: 0.024603, kld: 0.262316\n",
      "[23,     2] loss: 0.034323, kld: 0.335438\n",
      "[24,     2] loss: 0.029761, kld: 0.357860\n",
      "[25,     2] loss: 0.010403, kld: 0.334488\n",
      "[26,     2] loss: 0.012458, kld: 0.367533\n",
      "[27,     2] loss: 0.028940, kld: 0.355705\n",
      "[28,     2] loss: 0.022991, kld: 0.324294\n",
      "[29,     2] loss: 0.018718, kld: 0.333712\n",
      "[30,     2] loss: 0.023806, kld: 0.333981\n",
      "[31,     2] loss: 0.028192, kld: 0.296703\n",
      "[32,     2] loss: 0.019626, kld: 0.340727\n",
      "[33,     2] loss: 0.031925, kld: 0.324277\n",
      "[34,     2] loss: 0.037692, kld: 0.298450\n",
      "[35,     2] loss: 0.016263, kld: 0.339433\n",
      "[36,     2] loss: 0.023933, kld: 0.302627\n",
      "[37,     2] loss: 0.024158, kld: 0.285020\n",
      "[38,     2] loss: 0.023046, kld: 0.340107\n",
      "[39,     2] loss: 0.017362, kld: 0.315675\n",
      "[40,     2] loss: 0.013356, kld: 0.299215\n",
      "[41,     2] loss: 0.016564, kld: 0.350887\n",
      "[42,     2] loss: 0.035835, kld: 0.305157\n",
      "[43,     2] loss: 0.020685, kld: 0.323571\n",
      "[44,     2] loss: 0.018549, kld: 0.383788\n",
      "[45,     2] loss: 0.025457, kld: 0.410193\n",
      "[46,     2] loss: 0.041811, kld: 0.237444\n",
      "[47,     2] loss: 0.012285, kld: 0.346349\n",
      "[48,     2] loss: 0.010091, kld: 0.355405\n",
      "[49,     2] loss: 0.043429, kld: 0.318909\n",
      "[50,     2] loss: 0.025088, kld: 0.373147\n",
      "[51,     2] loss: 0.018673, kld: 0.385300\n",
      "[52,     2] loss: 0.020240, kld: 0.382832\n",
      "[53,     2] loss: 0.025257, kld: 0.340749\n",
      "[54,     2] loss: 0.009202, kld: 0.308212\n",
      "[55,     2] loss: 0.013216, kld: 0.325546\n",
      "[56,     2] loss: 0.027504, kld: 0.326656\n",
      "[57,     2] loss: 0.025725, kld: 0.339463\n",
      "[58,     2] loss: 0.025891, kld: 0.337471\n",
      "[59,     2] loss: 0.014526, kld: 0.325060\n",
      "[60,     2] loss: 0.052266, kld: 0.284381\n",
      "[61,     2] loss: 0.037952, kld: 0.286523\n",
      "[62,     2] loss: 0.024599, kld: 0.292984\n",
      "[63,     2] loss: 0.030947, kld: 0.371043\n",
      "[64,     2] loss: 0.052092, kld: 0.275277\n",
      "[65,     2] loss: 0.037764, kld: 0.360764\n",
      "[66,     2] loss: 0.021895, kld: 0.361403\n",
      "[67,     2] loss: 0.026722, kld: 0.279295\n",
      "[68,     2] loss: 0.026802, kld: 0.301615\n",
      "[69,     2] loss: 0.017835, kld: 0.318856\n",
      "[70,     2] loss: 0.022419, kld: 0.301271\n",
      "[71,     2] loss: 0.019847, kld: 0.378731\n",
      "[72,     2] loss: 0.028821, kld: 0.337623\n",
      "[73,     2] loss: 0.041403, kld: 0.290626\n",
      "[74,     2] loss: 0.035962, kld: 0.304955\n",
      "[75,     2] loss: 0.014924, kld: 0.351389\n",
      "[76,     2] loss: 0.015880, kld: 0.348397\n",
      "[77,     2] loss: 0.011083, kld: 0.336739\n",
      "[78,     2] loss: 0.009771, kld: 0.306001\n",
      "[79,     2] loss: 0.011340, kld: 0.262167\n",
      "[80,     2] loss: 0.032961, kld: 0.312605\n",
      "[81,     2] loss: 0.028654, kld: 0.331354\n",
      "[82,     2] loss: 0.019654, kld: 0.279711\n",
      "[83,     2] loss: 0.017349, kld: 0.319361\n",
      "[84,     2] loss: 0.040885, kld: 0.377145\n",
      "[85,     2] loss: 0.030324, kld: 0.271864\n",
      "[86,     2] loss: 0.031651, kld: 0.228513\n",
      "[87,     2] loss: 0.043519, kld: 0.279308\n",
      "[88,     2] loss: 0.015363, kld: 0.319764\n",
      "[89,     2] loss: 0.018268, kld: 0.314835\n",
      "[90,     2] loss: 0.029408, kld: 0.321637\n",
      "[91,     2] loss: 0.035538, kld: 0.302230\n",
      "[92,     2] loss: 0.030035, kld: 0.234987\n",
      "[93,     2] loss: 0.036786, kld: 0.269266\n",
      "[94,     2] loss: 0.044200, kld: 0.331439\n",
      "[95,     2] loss: 0.019155, kld: 0.290722\n",
      "[96,     2] loss: 0.032657, kld: 0.300447\n",
      "[97,     2] loss: 0.025161, kld: 0.346052\n",
      "[98,     2] loss: 0.011054, kld: 0.382796\n",
      "[99,     2] loss: 0.020423, kld: 0.321976\n",
      "[100,     2] loss: 0.029011, kld: 0.338874\n",
      "[101,     2] loss: 0.023153, kld: 0.292811\n",
      "[102,     2] loss: 0.035409, kld: 0.357823\n",
      "[103,     2] loss: 0.018645, kld: 0.269814\n",
      "[104,     2] loss: 0.014317, kld: 0.278592\n",
      "[105,     2] loss: 0.032447, kld: 0.311396\n",
      "[106,     2] loss: 0.039824, kld: 0.292227\n",
      "[107,     2] loss: 0.013812, kld: 0.363423\n",
      "[108,     2] loss: 0.025814, kld: 0.302536\n",
      "[109,     2] loss: 0.042494, kld: 0.303663\n",
      "[110,     2] loss: 0.025024, kld: 0.315165\n",
      "[111,     2] loss: 0.023432, kld: 0.274703\n",
      "[112,     2] loss: 0.020001, kld: 0.307777\n",
      "[113,     2] loss: 0.009134, kld: 0.279033\n",
      "[114,     2] loss: 0.021238, kld: 0.310177\n",
      "[115,     2] loss: 0.010913, kld: 0.368643\n",
      "[116,     2] loss: 0.027204, kld: 0.328039\n",
      "[117,     2] loss: 0.016644, kld: 0.312718\n",
      "[118,     2] loss: 0.026933, kld: 0.334713\n",
      "[119,     2] loss: 0.025027, kld: 0.339121\n",
      "[120,     2] loss: 0.024151, kld: 0.276836\n",
      "[121,     2] loss: 0.033609, kld: 0.288628\n",
      "[122,     2] loss: 0.010880, kld: 0.298926\n",
      "[123,     2] loss: 0.041202, kld: 0.268288\n",
      "[124,     2] loss: 0.025864, kld: 0.330466\n",
      "[125,     2] loss: 0.021802, kld: 0.399844\n",
      "[126,     2] loss: 0.039734, kld: 0.284211\n",
      "[127,     2] loss: 0.056612, kld: 0.257392\n",
      "[128,     2] loss: 0.051334, kld: 0.268787\n",
      "[129,     2] loss: 0.025789, kld: 0.296174\n",
      "[130,     2] loss: 0.022677, kld: 0.319030\n",
      "[131,     2] loss: 0.029853, kld: 0.325716\n",
      "[132,     2] loss: 0.021818, kld: 0.353075\n",
      "[133,     2] loss: 0.020219, kld: 0.348878\n",
      "[134,     2] loss: 0.021637, kld: 0.359662\n",
      "[135,     2] loss: 0.038622, kld: 0.336434\n",
      "[136,     2] loss: 0.023503, kld: 0.280245\n",
      "[137,     2] loss: 0.037294, kld: 0.379123\n",
      "[138,     2] loss: 0.020220, kld: 0.338576\n",
      "[139,     2] loss: 0.022731, kld: 0.272540\n",
      "[140,     2] loss: 0.028874, kld: 0.331168\n",
      "[141,     2] loss: 0.027841, kld: 0.348295\n",
      "[142,     2] loss: 0.046413, kld: 0.317317\n",
      "[143,     2] loss: 0.014575, kld: 0.319989\n",
      "[144,     2] loss: 0.028016, kld: 0.291575\n",
      "[145,     2] loss: 0.016598, kld: 0.342369\n",
      "[146,     2] loss: 0.052554, kld: 0.319864\n",
      "[147,     2] loss: 0.020809, kld: 0.305950\n",
      "[148,     2] loss: 0.013635, kld: 0.374371\n",
      "[149,     2] loss: 0.015911, kld: 0.257865\n",
      "[150,     2] loss: 0.029587, kld: 0.327115\n",
      "[151,     2] loss: 0.030285, kld: 0.325256\n",
      "[152,     2] loss: 0.029399, kld: 0.285549\n",
      "[153,     2] loss: 0.032864, kld: 0.269382\n",
      "[154,     2] loss: 0.009187, kld: 0.300772\n",
      "[155,     2] loss: 0.033402, kld: 0.267420\n",
      "[156,     2] loss: 0.028941, kld: 0.291303\n",
      "[157,     2] loss: 0.043806, kld: 0.273901\n",
      "[158,     2] loss: 0.015668, kld: 0.317880\n",
      "[159,     2] loss: 0.020720, kld: 0.302250\n",
      "[160,     2] loss: 0.027650, kld: 0.316880\n",
      "[161,     2] loss: 0.015474, kld: 0.279006\n",
      "[162,     2] loss: 0.013295, kld: 0.282350\n",
      "[163,     2] loss: 0.024231, kld: 0.307716\n",
      "[164,     2] loss: 0.016022, kld: 0.266988\n",
      "[165,     2] loss: 0.023849, kld: 0.289839\n",
      "[166,     2] loss: 0.009668, kld: 0.333651\n",
      "[167,     2] loss: 0.030992, kld: 0.260053\n",
      "[168,     2] loss: 0.028564, kld: 0.334672\n",
      "[169,     2] loss: 0.022736, kld: 0.307056\n",
      "[170,     2] loss: 0.026461, kld: 0.311984\n",
      "[171,     2] loss: 0.034103, kld: 0.247037\n",
      "[172,     2] loss: 0.017416, kld: 0.295590\n",
      "[173,     2] loss: 0.018051, kld: 0.283100\n",
      "[174,     2] loss: 0.021553, kld: 0.340589\n",
      "[175,     2] loss: 0.033104, kld: 0.372142\n",
      "[176,     2] loss: 0.019629, kld: 0.300634\n",
      "[177,     2] loss: 0.034366, kld: 0.273095\n",
      "[178,     2] loss: 0.021573, kld: 0.370366\n",
      "[179,     2] loss: 0.043446, kld: 0.298273\n",
      "[180,     2] loss: 0.017615, kld: 0.251515\n",
      "[181,     2] loss: 0.044924, kld: 0.292714\n",
      "[182,     2] loss: 0.017212, kld: 0.302002\n",
      "[183,     2] loss: 0.047350, kld: 0.257310\n",
      "[184,     2] loss: 0.036712, kld: 0.354279\n",
      "[185,     2] loss: 0.035011, kld: 0.270278\n",
      "[186,     2] loss: 0.016087, kld: 0.308569\n",
      "[187,     2] loss: 0.031039, kld: 0.323442\n",
      "[188,     2] loss: 0.018179, kld: 0.334448\n",
      "[189,     2] loss: 0.038880, kld: 0.327833\n",
      "[190,     2] loss: 0.037768, kld: 0.358747\n",
      "[191,     2] loss: 0.010229, kld: 0.316184\n",
      "[192,     2] loss: 0.027158, kld: 0.250713\n",
      "[193,     2] loss: 0.032489, kld: 0.344523\n",
      "[194,     2] loss: 0.019204, kld: 0.275549\n",
      "[195,     2] loss: 0.019697, kld: 0.262002\n",
      "[196,     2] loss: 0.034240, kld: 0.284012\n",
      "[197,     2] loss: 0.029650, kld: 0.261981\n",
      "[198,     2] loss: 0.028923, kld: 0.289996\n",
      "[199,     2] loss: 0.037307, kld: 0.277836\n",
      "[200,     2] loss: 0.012261, kld: 0.306383\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    total_loss = 0.0\n",
    "    for ind, (x, y) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model(x.cuda()).squeeze(), y.cuda())\n",
    "        kld = model.kld()\n",
    "        (loss + kld).backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        total_loss += loss.item()\n",
    "        if ind % 50 == 1:    # print every 100 mini-batches\n",
    "            print('[%d, %5d] loss: %.6f, kld: %.6f' %\n",
    "                  (epoch + 1, ind + 1, running_loss / 50, kld))\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1dcf8c-c24e-4e5e-a478-964effbceaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    total_loss = 0.0\n",
    "    for i, data in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        model_state_dict = copy.deepcopy(net_fn.state_dict())\n",
    "        #loss = - log_posterior_fn(model_state_dict, data)\n",
    "        loss = - log_posterior_fn(net_fn, model_state_dict, data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler1.step()\n",
    "        running_loss += loss.item()\n",
    "        total_loss += loss.item()\n",
    "        if i % 100 == 99:    # print every 100 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "    model_state_dict = copy.deepcopy(net_fn.state_dict())\n",
    "    test_acc, all_test_probs = evaluate_fn(test_loader, model_state_dict)\n",
    "    scheduler2.step(test_acc)\n",
    "    \n",
    "    print(\"Epoch {}\".format(epoch))\n",
    "    print(\"\\tAverage loss: {}\".format(total_loss / epoch_steps))\n",
    "    print(\"\\tTest accuracy: {}\".format(test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
