{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4709797a-9db8-4650-b912-bc6201868d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import tqdm\n",
    "import math\n",
    "import inspect\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from adabelief_pytorch import AdaBelief\n",
    "\n",
    "from wrapper import Variational_Flipout, Variational_LRT\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d3b7e0b-81fe-4af7-9bed-2e47644d25d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "train_set = datasets.CIFAR10(\n",
    "    '~/DATA', train=True, download=True,\n",
    "    transform=transform)\n",
    "test_set = datasets.CIFAR10(\n",
    "    '~/DATA', train=False,\n",
    "    transform=transform)\n",
    "train_loader = DataLoader(train_set, batch_size = 128, shuffle=True, num_workers = 16)\n",
    "test_loader = DataLoader(test_set, batch_size = 128, shuffle=False, num_workers = 16)\n",
    "\n",
    "def mul_sign(x) -> torch.Tensor:\n",
    "    #Best performance on several experiments\n",
    "    return x.mul(torch.empty(x.shape, device = x.device).uniform_(-1,1).sign())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1d3e23e-5196-4316-a7f5-f1ccecf4c4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(net, self).__init__()\n",
    "        self.b1 = nn.BatchNorm2d(3)\n",
    "        self.l1 = nn.Conv2d(3, 64, kernel_size=3)\n",
    "        self.p1 = nn.MaxPool2d((2,2))\n",
    "        self.b2 = nn.BatchNorm2d(64)\n",
    "        self.l2 = nn.Conv2d(64, 64, kernel_size=3)\n",
    "        self.p2 = nn.MaxPool2d((2,2))\n",
    "        self.b3 = nn.BatchNorm2d(64)\n",
    "        self.l3 = nn.Conv2d(64, 64, kernel_size=3)\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        self.b4 = nn.BatchNorm1d(1024)\n",
    "        self.l4 = nn.Linear(1024, 64)\n",
    "        self.b5 = nn.BatchNorm1d(64)\n",
    "        self.l5 = nn.Linear(64, 10, bias = True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.b1(x)\n",
    "        x = self.l1(x)\n",
    "        x = F.silu(x)\n",
    "        x = self.p1(x)\n",
    "        x = self.b2(x)\n",
    "        x = self.l2(x)\n",
    "        x = F.silu(x)\n",
    "        x = self.p2(x)\n",
    "        x = self.b3(x)\n",
    "        x = self.l3(x)\n",
    "        x = F.silu(x)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        x = self.b4(x)\n",
    "        x = self.l4(x)\n",
    "        x = F.silu(x)\n",
    "        x = self.b5(x)\n",
    "        x = self.l5(x)\n",
    "                \n",
    "        return x\n",
    "    \n",
    "    def kld(self):\n",
    "        return 0.0\n",
    "    \n",
    "class net_Flipout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(net_Flipout, self).__init__()\n",
    "        self.b1 = nn.BatchNorm2d(3)\n",
    "        self.l1 = Variational_Flipout(nn.Conv2d(3, 64, kernel_size=3))\n",
    "        self.p1 = nn.MaxPool2d((2,2))\n",
    "        self.b2 = nn.BatchNorm2d(64)\n",
    "        self.l2 = Variational_Flipout(nn.Conv2d(64, 64, kernel_size=3))\n",
    "        self.p2 = nn.MaxPool2d((2,2))\n",
    "        self.b3 = nn.BatchNorm2d(64)\n",
    "        self.l3 = Variational_Flipout(nn.Conv2d(64, 64, kernel_size=3))\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        self.b4 = nn.BatchNorm1d(1024)\n",
    "        self.l4 = Variational_Flipout(nn.Linear(1024, 64))\n",
    "        self.b5 = nn.BatchNorm1d(64)\n",
    "        self.l5 = Variational_Flipout(nn.Linear(64, 10, bias = True))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.b1(x)\n",
    "        x = self.l1(x)\n",
    "        x = F.silu(x)\n",
    "        x = self.p1(x)\n",
    "        x = self.b2(x)\n",
    "        x = self.l2(x)\n",
    "        x = F.silu(x)\n",
    "        x = self.p2(x)\n",
    "        x = self.b3(x)\n",
    "        x = self.l3(x)\n",
    "        x = F.silu(x)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        x = self.b4(x)\n",
    "        x = self.l4(x)\n",
    "        x = F.silu(x)\n",
    "        x = self.b5(x)\n",
    "        x = self.l5(x)\n",
    "                \n",
    "        return x\n",
    "    \n",
    "    def kld(self):\n",
    "        sum_kl = 0.0\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, Variational_Flipout):\n",
    "                sum_kl += module.kld()\n",
    "        return sum_kl\n",
    "    \n",
    "class net_LRT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(net_LRT, self).__init__()\n",
    "        self.b1 = nn.BatchNorm2d(3)\n",
    "        self.l1 = Variational_LRT(nn.Conv2d(3, 64, kernel_size=3))\n",
    "        self.p1 = nn.MaxPool2d((2,2))\n",
    "        self.b2 = nn.BatchNorm2d(64)\n",
    "        self.l2 = Variational_LRT(nn.Conv2d(64, 64, kernel_size=3))\n",
    "        self.p2 = nn.MaxPool2d((2,2))\n",
    "        self.b3 = nn.BatchNorm2d(64)\n",
    "        self.l3 = Variational_LRT(nn.Conv2d(64, 64, kernel_size=3))\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        self.b4 = nn.BatchNorm1d(1024)\n",
    "        self.l4 = Variational_LRT(nn.Linear(1024, 64))\n",
    "        self.b5 = nn.BatchNorm1d(64)\n",
    "        self.l5 = Variational_LRT(nn.Linear(64, 10, bias = True))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.b1(x)\n",
    "        x = self.l1(x)\n",
    "        x = F.silu(x)\n",
    "        x = self.p1(x)\n",
    "        x = self.b2(x)\n",
    "        x = self.l2(x)\n",
    "        x = F.silu(x)\n",
    "        x = self.p2(x)\n",
    "        x = self.b3(x)\n",
    "        x = self.l3(x)\n",
    "        x = F.silu(x)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        x = self.b4(x)\n",
    "        x = self.l4(x)\n",
    "        x = F.silu(x)\n",
    "        x = self.b5(x)\n",
    "        x = self.l5(x)\n",
    "                \n",
    "        return x\n",
    "    \n",
    "    def kld(self):\n",
    "        sum_kl = 0.0\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, Variational_LRT):\n",
    "                sum_kl += module.kld()\n",
    "        return sum_kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e1956c6a-e274-4cdd-a111-121b72cd9b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: 0.010562/0.008737, acc: 0.507900/0.604900, kld: 0.000000\n",
      "[2] loss: 0.007412/0.007241, acc: 0.664460/0.679000, kld: 0.000000\n",
      "[3] loss: 0.006138/0.006675, acc: 0.724960/0.707000, kld: 0.000000\n",
      "[4] loss: 0.005308/0.006092, acc: 0.761660/0.733300, kld: 0.000000\n",
      "[5] loss: 0.004744/0.005840, acc: 0.787560/0.747200, kld: 0.000000\n",
      "[6] loss: 0.004274/0.005975, acc: 0.806300/0.746000, kld: 0.000000\n",
      "[7] loss: 0.003853/0.005820, acc: 0.828420/0.757800, kld: 0.000000\n",
      "[8] loss: 0.003507/0.006040, acc: 0.843000/0.748800, kld: 0.000000\n",
      "[9] loss: 0.003193/0.006187, acc: 0.854360/0.751100, kld: 0.000000\n",
      "[10] loss: 0.002849/0.006514, acc: 0.869460/0.749300, kld: 0.000000\n",
      "[11] loss: 0.002678/0.006465, acc: 0.876800/0.752000, kld: 0.000000\n",
      "[12] loss: 0.002365/0.007127, acc: 0.891540/0.736700, kld: 0.000000\n",
      "[13] loss: 0.002169/0.007379, acc: 0.899960/0.743500, kld: 0.000000\n",
      "[14] loss: 0.002019/0.007738, acc: 0.907500/0.739200, kld: 0.000000\n",
      "[15] loss: 0.001904/0.007774, acc: 0.911720/0.747600, kld: 0.000000\n",
      "[16] loss: 0.001721/0.008255, acc: 0.919980/0.749000, kld: 0.000000\n",
      "[17] loss: 0.001553/0.008834, acc: 0.929660/0.745400, kld: 0.000000\n",
      "[18] loss: 0.001504/0.009126, acc: 0.930420/0.733700, kld: 0.000000\n",
      "[19] loss: 0.001373/0.009361, acc: 0.937320/0.740400, kld: 0.000000\n",
      "[20] loss: 0.001284/0.009104, acc: 0.941120/0.749300, kld: 0.000000\n",
      "[21] loss: 0.001181/0.009632, acc: 0.946060/0.732600, kld: 0.000000\n",
      "[22] loss: 0.001117/0.010141, acc: 0.949320/0.739700, kld: 0.000000\n",
      "[23] loss: 0.001141/0.009839, acc: 0.947400/0.747300, kld: 0.000000\n",
      "[24] loss: 0.000986/0.010491, acc: 0.954760/0.748900, kld: 0.000000\n",
      "[25] loss: 0.000894/0.010709, acc: 0.959180/0.740100, kld: 0.000000\n",
      "[26] loss: 0.000918/0.010994, acc: 0.958560/0.735100, kld: 0.000000\n",
      "[27] loss: 0.000867/0.011222, acc: 0.961000/0.736900, kld: 0.000000\n",
      "[28] loss: 0.000804/0.011321, acc: 0.963220/0.740500, kld: 0.000000\n",
      "[29] loss: 0.000796/0.011594, acc: 0.963460/0.744500, kld: 0.000000\n",
      "[30] loss: 0.000744/0.012062, acc: 0.965500/0.738200, kld: 0.000000\n",
      "[31] loss: 0.000756/0.011931, acc: 0.965340/0.742100, kld: 0.000000\n",
      "[32] loss: 0.000664/0.012148, acc: 0.969960/0.739200, kld: 0.000000\n",
      "[33] loss: 0.000749/0.012294, acc: 0.966380/0.739500, kld: 0.000000\n",
      "[34] loss: 0.000698/0.012236, acc: 0.968760/0.738800, kld: 0.000000\n",
      "[35] loss: 0.000630/0.012234, acc: 0.970960/0.744700, kld: 0.000000\n",
      "[36] loss: 0.000580/0.012636, acc: 0.973300/0.742900, kld: 0.000000\n",
      "[37] loss: 0.000584/0.012464, acc: 0.973920/0.743300, kld: 0.000000\n",
      "[38] loss: 0.000576/0.012727, acc: 0.973920/0.745000, kld: 0.000000\n",
      "[39] loss: 0.000618/0.012671, acc: 0.973180/0.747000, kld: 0.000000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/.tmp/ipykernel_37248/512008044.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mkld\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m#break\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mtrain_acc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/jupyter_public/lib/python3.9/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/research/__common_modules__/Adabelief-Optimizer/pypi_packages/adabelief_pytorch0.2.1/adabelief_pytorch/AdaBelief.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m                 \u001b[0;31m# Update first and second moment running average\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m                 \u001b[0mgrad_residual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mexp_avg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0mexp_avg_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mgrad_residual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_residual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = net().cuda()\n",
    "num_epochs = 200\n",
    "lr = 1e-2\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = AdaBelief(\n",
    "    model.parameters(),\n",
    "    lr=lr, betas=(0.9, 0.999), eps=1e-8, weight_decouple = True, rectify = False,\n",
    "    print_change_log = False,\n",
    ")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "    test_loss = 0.0\n",
    "    test_acc = 0.0\n",
    "    \n",
    "    model.train()\n",
    "    for ind, (x, y) in enumerate(train_loader):\n",
    "        y = y.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(x.cuda())\n",
    "        loss = criterion(pred, y)\n",
    "        kld = model.kld()\n",
    "        #loss.backward()\n",
    "        (loss + 0.1*kld).backward()\n",
    "        #break\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        train_acc += (pred.argmax(dim = -1) == y).float().sum()\n",
    "    train_loss /= len(train_set)\n",
    "    train_acc /= len(train_set)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for ind, (x, y) in enumerate(test_loader):\n",
    "            y = y.cuda()\n",
    "            pred = model(x.cuda())\n",
    "            loss = criterion(pred, y)\n",
    "            test_loss += loss.item()\n",
    "            test_acc += (pred.argmax(dim = -1) == y).float().sum()\n",
    "        test_loss /= len(test_set)\n",
    "        test_acc /= len(test_set)\n",
    "        \n",
    "        kld = model.kld()\n",
    "        \n",
    "    print(\n",
    "        '[%d] loss: %.6f/%.6f, acc: %.6f/%.6f, kld: %.6f' %\n",
    "        (epoch + 1, train_loss, test_loss, train_acc, test_acc, kld)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dcf5b06f-f811-479e-a9f9-a6bb2bb69549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: 0.010948/0.009657, acc: 0.489980/0.561900, kld: 7.432627\n",
      "[2] loss: 0.008287/0.008162, acc: 0.625160/0.629200, kld: 6.963572\n",
      "[3] loss: 0.007275/0.007871, acc: 0.671960/0.647400, kld: 6.588394\n",
      "[4] loss: 0.006841/0.007433, acc: 0.692320/0.675400, kld: 6.341869\n",
      "[5] loss: 0.006627/0.007690, acc: 0.704660/0.659800, kld: 6.215690\n",
      "[6] loss: 0.006416/0.007749, acc: 0.714460/0.660000, kld: 6.158609\n",
      "[7] loss: 0.006246/0.007465, acc: 0.722040/0.675400, kld: 6.129203\n",
      "[8] loss: 0.006022/0.007735, acc: 0.733040/0.667700, kld: 6.107191\n",
      "[9] loss: 0.005899/0.007460, acc: 0.740680/0.682300, kld: 6.091041\n",
      "[10] loss: 0.005714/0.007449, acc: 0.747940/0.688500, kld: 6.074485\n",
      "[11] loss: 0.005553/0.007512, acc: 0.755380/0.681100, kld: 6.058074\n",
      "[12] loss: 0.005419/0.007310, acc: 0.761800/0.689100, kld: 6.042441\n",
      "[13] loss: 0.005227/0.007118, acc: 0.769900/0.698000, kld: 6.026711\n",
      "[14] loss: 0.005134/0.007308, acc: 0.773940/0.688800, kld: 6.011161\n",
      "[15] loss: 0.004997/0.007223, acc: 0.779460/0.698800, kld: 5.994944\n",
      "[16] loss: 0.004882/0.007800, acc: 0.784300/0.682800, kld: 5.978986\n",
      "[17] loss: 0.004812/0.007474, acc: 0.786240/0.691100, kld: 5.963178\n",
      "[18] loss: 0.004641/0.007400, acc: 0.794820/0.699400, kld: 5.945674\n",
      "[19] loss: 0.004661/0.007333, acc: 0.795120/0.700200, kld: 5.930016\n",
      "[20] loss: 0.004517/0.007548, acc: 0.801220/0.694200, kld: 5.913289\n",
      "[21] loss: 0.004508/0.007244, acc: 0.801540/0.706700, kld: 5.896439\n",
      "[22] loss: 0.004370/0.007363, acc: 0.808700/0.703000, kld: 5.878530\n",
      "[23] loss: 0.004364/0.007598, acc: 0.806840/0.696700, kld: 5.863266\n",
      "[24] loss: 0.004252/0.007358, acc: 0.811560/0.704500, kld: 5.846512\n",
      "[25] loss: 0.004211/0.007241, acc: 0.814160/0.707400, kld: 5.828960\n",
      "[26] loss: 0.004118/0.007714, acc: 0.816720/0.698700, kld: 5.810517\n",
      "[27] loss: 0.004116/0.007554, acc: 0.818260/0.700000, kld: 5.793502\n",
      "[28] loss: 0.004046/0.007435, acc: 0.821740/0.706000, kld: 5.775011\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/.tmp/ipykernel_37248/3260137306.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mkld\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;31m#loss.backward()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mkld\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = net_Flipout().cuda()\n",
    "num_epochs = 200\n",
    "lr = 1e-2\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = AdaBelief(\n",
    "    model.parameters(),\n",
    "    lr=lr, betas=(0.9, 0.999), eps=1e-8, weight_decouple = True, rectify=False,\n",
    "    print_change_log = False,\n",
    ")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "    test_loss = 0.0\n",
    "    test_acc = 0.0\n",
    "    \n",
    "    model.train()\n",
    "    for ind, (x, y) in enumerate(train_loader):\n",
    "        y = y.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(x.cuda())\n",
    "        loss = criterion(pred, y)\n",
    "        kld = model.kld()\n",
    "        #loss.backward()\n",
    "        (loss + kld).backward()\n",
    "        #break\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        train_acc += (pred.argmax(dim = -1) == y).float().sum()\n",
    "    train_loss /= len(train_set)\n",
    "    train_acc /= len(train_set)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for ind, (x, y) in enumerate(test_loader):\n",
    "            y = y.cuda()\n",
    "            pred = model(x.cuda())\n",
    "            loss = criterion(pred, y)\n",
    "            test_loss += loss.item()\n",
    "            test_acc += (pred.argmax(dim = -1) == y).float().sum()\n",
    "        test_loss /= len(test_set)\n",
    "        test_acc /= len(test_set)\n",
    "        \n",
    "        kld = model.kld()\n",
    "        \n",
    "    print(\n",
    "        '[%d] loss: %.6f/%.6f, acc: %.6f/%.6f, kld: %.6f' %\n",
    "        (epoch + 1, train_loss, test_loss, train_acc, test_acc, kld)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3542eed2-5390-4a05-ba05-6a312f456705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight decoupling enabled in AdaBelief\n",
      "[1] loss: 0.010528/0.009041, acc: 0.512180/0.596900, kld: 10.473148\n",
      "[2] loss: 0.007785/0.007633, acc: 0.647540/0.657500, kld: 9.897487\n",
      "[3] loss: 0.006807/0.007258, acc: 0.693140/0.681600, kld: 9.409855\n",
      "[4] loss: 0.006280/0.007739, acc: 0.718000/0.665600, kld: 9.050241\n",
      "[5] loss: 0.005991/0.007049, acc: 0.731180/0.693300, kld: 8.816916\n",
      "[6] loss: 0.005771/0.007008, acc: 0.744780/0.701400, kld: 8.663939\n",
      "[7] loss: 0.005614/0.007066, acc: 0.751280/0.704900, kld: 8.552006\n",
      "[8] loss: 0.005427/0.007036, acc: 0.758900/0.703700, kld: 8.466328\n",
      "[9] loss: 0.005167/0.007255, acc: 0.772160/0.695200, kld: 8.397057\n",
      "[10] loss: 0.005131/0.006932, acc: 0.772020/0.705600, kld: 8.344603\n",
      "[11] loss: 0.004941/0.007595, acc: 0.780800/0.692800, kld: 8.302938\n",
      "[12] loss: 0.004853/0.007040, acc: 0.786220/0.713200, kld: 8.269974\n",
      "[13] loss: 0.004674/0.007468, acc: 0.793720/0.691400, kld: 8.242685\n",
      "[14] loss: 0.004578/0.007124, acc: 0.799560/0.705700, kld: 8.219312\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/.tmp/ipykernel_15288/1140629941.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mkld\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m#loss.backward()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mkld\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/jupyter_public/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/jupyter_public/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = net_LRT().cuda()\n",
    "num_epochs = 200\n",
    "lr = 1e-2\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = AdaBelief(\n",
    "    model.parameters(),\n",
    "    lr=lr, betas=(0.9, 0.999), eps=1e-8, weight_decouple = True, rectify=False,\n",
    "    print_change_log = False,\n",
    ")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "    test_loss = 0.0\n",
    "    test_acc = 0.0\n",
    "    \n",
    "    model.train()\n",
    "    for ind, (x, y) in enumerate(train_loader):\n",
    "        y = y.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(x.cuda())\n",
    "        loss = criterion(pred, y)\n",
    "        kld = model.kld()\n",
    "        #loss.backward()\n",
    "        (loss + kld).backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        train_acc += (pred.argmax(dim = -1) == y).float().sum()\n",
    "    train_loss /= len(train_set)\n",
    "    train_acc /= len(train_set)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for ind, (x, y) in enumerate(test_loader):\n",
    "            y = y.cuda()\n",
    "            pred = model(x.cuda())\n",
    "            loss = criterion(pred, y)\n",
    "            test_loss += loss.item()\n",
    "            test_acc += (pred.argmax(dim = -1) == y).float().sum()\n",
    "        test_loss /= len(test_set)\n",
    "        test_acc /= len(test_set)\n",
    "        \n",
    "        kld = model.kld()\n",
    "        \n",
    "    print(\n",
    "        '[%d] loss: %.6f/%.6f, acc: %.6f/%.6f, kld: %.6f' %\n",
    "        (epoch + 1, train_loss, test_loss, train_acc, test_acc, kld)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db8a777-b9b9-4212-ba95-18551be31948",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
